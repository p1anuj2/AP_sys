{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cceeedb-1b83-4dc8-96c9-9abc9a64e3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale (Top View): 0.028409090909090908 cm/px\n",
      "Scale (Left View): 0.025510204081632654 cm/px\n",
      "Scale (Right View): 0.026041666666666668 cm/px\n",
      "Object Length: 4.017652165832656 cm\n",
      "Object Width: 4.017652165832656 cm\n",
      "Object Height: 4.598917443193864 cm\n",
      "Estimated Object Volume: 74.23355893585135 cubic cm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load images\n",
    "top_view_image = cv2.imread(r\"F:\\Anuj\\image\\TopView.jpg\")\n",
    "left_view_image = cv2.imread(r\"F:\\Anuj\\image\\LeftView.jpg\")\n",
    "right_view_image = cv2.imread(r\"F:\\Anuj\\image\\RightView.jpg\")\n",
    "\n",
    "# Known dimensions of the reference coin (in cm)\n",
    "coin_diameter_cm = 2.5\n",
    "coin_thickness_cm = 0.2\n",
    "\n",
    "def detect_coin(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (11, 11), 0)\n",
    "    circles = cv2.HoughCircles(blurred, cv2.HOUGH_GRADIENT, dp=1.2, minDist=50,\n",
    "                               param1=50, param2=30, minRadius=10, maxRadius=100)\n",
    "    if circles is not None:\n",
    "        circles = np.round(circles[0, :]).astype(\"int\")\n",
    "        for (x, y, r) in circles:\n",
    "            # We are making a rough assumption on coin radius range based on the expected coin size\n",
    "            if 10 < r < 50:  # Adjust these values if needed\n",
    "                return (x, y, r)\n",
    "    return None\n",
    "\n",
    "def calculate_scale(coin_radius_px, coin_diameter_cm):\n",
    "    return coin_diameter_cm / (2 * coin_radius_px)\n",
    "\n",
    "def estimate_dimension(scale, point1, point2):\n",
    "    distance_px = np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "    return distance_px * scale\n",
    "\n",
    "# Detect coin in top view to establish scale\n",
    "coin_top = detect_coin(top_view_image)\n",
    "if coin_top is None:\n",
    "    print(\"Coin not detected in top view image\")\n",
    "else:\n",
    "    coin_radius_top_px = coin_top[2]\n",
    "    scale_top = calculate_scale(coin_radius_top_px, coin_diameter_cm)\n",
    "    print(f\"Scale (Top View): {scale_top} cm/px\")\n",
    "\n",
    "# Detect coin in left view to establish scale\n",
    "coin_left = detect_coin(left_view_image)\n",
    "if coin_left is None:\n",
    "    print(\"Coin not detected in left view image\")\n",
    "else:\n",
    "    coin_radius_left_px = coin_left[2]\n",
    "    scale_left = calculate_scale(coin_radius_left_px, coin_diameter_cm)\n",
    "    print(f\"Scale (Left View): {scale_left} cm/px\")\n",
    "\n",
    "# Detect coin in right view to establish scale\n",
    "coin_right = detect_coin(right_view_image)\n",
    "if coin_right is None:\n",
    "    print(\"Coin not detected in right view image\")\n",
    "else:\n",
    "    coin_radius_right_px = coin_right[2]\n",
    "    scale_right = calculate_scale(coin_radius_right_px, coin_diameter_cm)\n",
    "    print(f\"Scale (Right View): {scale_right} cm/px\")\n",
    "\n",
    "# Check if all scales are defined\n",
    "if 'scale_top' in locals() and 'scale_left' in locals() and 'scale_right' in locals():\n",
    "    # Assuming object spans from (x1, y1) to (x2, y2) in the top view\n",
    "    object_top_corners = [(50, 50), (150, 150)]  # Example points; replace with actual object detection\n",
    "    object_length_px = estimate_dimension(scale_top, object_top_corners[0], object_top_corners[1])\n",
    "    object_width_px = estimate_dimension(scale_top, object_top_corners[0], object_top_corners[1])\n",
    "    print(f\"Object Length: {object_length_px} cm\")\n",
    "    print(f\"Object Width: {object_width_px} cm\")\n",
    "\n",
    "    # Assuming object spans from (x1, y1) to (x2, y2) in the left view\n",
    "    object_left_corners = [(50, 50), (150, 200)]  # Example points; replace with actual object detection\n",
    "    object_height_px = estimate_dimension(scale_left, object_left_corners[0], object_left_corners[1])\n",
    "    print(f\"Object Height: {object_height_px} cm\")\n",
    "\n",
    "    # Calculate volume (assuming object is a rectangular prism)\n",
    "    object_volume = object_length_px * object_width_px * object_height_px\n",
    "    print(f\"Estimated Object Volume: {object_volume} cubic cm\")\n",
    "\n",
    "    # Visualization (Optional)\n",
    "    cv2.circle(top_view_image, (coin_top[0], coin_top[1]), coin_top[2], (0, 255, 0), 2)\n",
    "    cv2.circle(left_view_image, (coin_left[0], coin_left[1]), coin_left[2], (0, 255, 0), 2)\n",
    "    cv2.circle(right_view_image, (coin_right[0], coin_right[1]), coin_right[2], (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Top View with Coin\", top_view_image)\n",
    "    cv2.imshow(\"Left View with Coin\", left_view_image)\n",
    "    cv2.imshow(\"Right View with Coin\", right_view_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(\"Could not detect the coin in one or more views, hence volume estimation cannot proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b90c5c7f-1961-4241-8000-9284b5202f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated distance to the object: 0.005773502691896257 meters\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "\n",
    "def estimate_depth(image_path, apparent_size_pixels, baseline_distance_meters, angle_degrees):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize image for better visualization (optional)\n",
    "    scale_percent = 50  # adjust as needed\n",
    "    width = int(image.shape[1] * scale_percent / 100)\n",
    "    height = int(image.shape[0] * scale_percent / 100)\n",
    "    resized_image = cv2.resize(image, (width, height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Display the resized image\n",
    "    cv2.imshow('Resized Image', resized_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Convert angle to radians\n",
    "    angle_radians = math.radians(angle_degrees)\n",
    "    \n",
    "    # Calculate distance using trigonometry\n",
    "    distance = baseline_distance_meters * math.tan(angle_radians) / (apparent_size_pixels / 2)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "# Example usage:\n",
    "image_path = r\"F:\\Anuj\\image\\LeftView.jpg\"  # Replace with your image file path\n",
    "apparent_size_pixels = 200  # Example: measured apparent size of the object in pixels\n",
    "baseline_distance_meters = 1.0  # Example: known distance between two points on the object in meters\n",
    "angle_degrees = 30.0  # Example: angle formed by the lines from the camera to the two points\n",
    "\n",
    "estimated_distance = estimate_depth(image_path, apparent_size_pixels, baseline_distance_meters, angle_degrees)\n",
    "print(f\"Estimated distance to the object: {estimated_distance} meters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7c56ce4-0369-4f9d-856a-c000b344acb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated distance to the object: 6.49 meters\n",
      "Estimated volume of the object: 0.00 cubic meters\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def estimate_depth_and_volume(image_path, object_width_cm, object_height_cm, focal_length_pixels):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not read the image at {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Threshold the image to get binary image\n",
    "    _, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Assume the largest contour is the object\n",
    "    if len(contours) == 0:\n",
    "        print(\"Error: No object detected in the image.\")\n",
    "        return None, None\n",
    "    \n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    \n",
    "    # Calculate apparent size in pixels (assuming the object is rectangular)\n",
    "    apparent_size_pixels = max(w, h)\n",
    "    \n",
    "    # Estimate depth using perspective properties\n",
    "    object_width_pixels = w\n",
    "    object_height_pixels = h\n",
    "    \n",
    "    distance = (object_width_cm * focal_length_pixels) / object_width_pixels\n",
    "    \n",
    "    # Estimate volume assuming object is a rectangular prism\n",
    "    object_volume = (object_width_cm / 100) * (object_height_cm / 100) * (distance / 100)  # Convert cm to meters\n",
    "    \n",
    "    return distance, object_volume\n",
    "\n",
    "# Example usage:\n",
    "image_path = r\"F:\\Anuj\\image\\LeftView.jpg\"  # Replace with your image file path\n",
    "object_width_cm = 4  # Example: width of the object in centimeters\n",
    "object_height_cm = 15  # Example: height of the object in centimeters\n",
    "focal_length_pixels = 480  # Example: focal length of the camera in pixels\n",
    "\n",
    "estimated_distance, estimated_volume = estimate_depth_and_volume(image_path, object_width_cm, object_height_cm, focal_length_pixels)\n",
    "\n",
    "if estimated_distance is not None and estimated_volume is not None:\n",
    "    print(f\"Estimated distance to the object: {estimated_distance:.2f} meters\")\n",
    "    print(f\"Estimated volume of the object: {estimated_volume:.2f} cubic meters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36b95ded-eae3-4fb8-846a-e4aa52f64fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\anuj\\anaconda3\\envs\\ap-anuj\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\anuj\\anaconda3\\envs\\ap-anuj\\lib\\site-packages (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\anuj\\anaconda3\\envs\\ap-anuj\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\anuj\\anaconda3\\envs\\ap-anuj\\lib\\site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\anuj\\anaconda3\\envs\\ap-anuj\\lib\\site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.1 in c:\\users\\anuj\\anaconda3\\envs\\ap-anuj\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\users\\anuj\\anaconda3\\envs\\ap-anuj\\lib\\site-packages (from scikit-image) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\anuj\\anaconda3\\envs\\ap-anuj\\lib\\site-packages (from scikit-image) (2024.5.22)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\anuj\\anaconda3\\envs\\ap-anuj\\lib\\site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\anuj\\anaconda3\\envs\\ap-anuj\\lib\\site-packages (from scikit-image) (0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc231370-fc31-4596-a5a7-d5cb5856eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.segmentation import slic\n",
    "from skimage.measure import regionprops\n",
    "from skimage.color import label2rgb\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(r\"F:\\Anuj\\DiabTrain\\biriyani\\biriyanitrain (67).jpg\")\n",
    "\n",
    "# Step 1: Selective Search\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "ss.setBaseImage(image)\n",
    "ss.switchToSelectiveSearchFast()\n",
    "rects = ss.process()\n",
    "\n",
    "# Draw initial bounding boxes\n",
    "output = image.copy()\n",
    "for (x, y, w, h) in rects[:100]:\n",
    "    cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow(\"Initial Bounding Boxes\", output)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Step 2: Bounding Box Clustering\n",
    "boxes = np.array(rects[:100])\n",
    "db = DBSCAN(eps=30, min_samples=2).fit(boxes)\n",
    "labels = db.labels_\n",
    "\n",
    "# Draw clustered bounding boxes\n",
    "for label in set(labels):\n",
    "    if label == -1:\n",
    "        continue\n",
    "    mask = (labels == label)\n",
    "    cluster_boxes = boxes[mask]\n",
    "    x1 = np.min(cluster_boxes[:, 0])\n",
    "    y1 = np.min(cluster_boxes[:, 1])\n",
    "    x2 = np.max(cluster_boxes[:, 0] + cluster_boxes[:, 2])\n",
    "    y2 = np.max(cluster_boxes[:, 1] + cluster_boxes[:, 3])\n",
    "    cv2.rectangle(output, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "cv2.imshow(\"Clustered Bounding Boxes\", output)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Step 3: Saliency Maps\n",
    "# Using OpenCV's saliency API\n",
    "saliency = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
    "(success, saliency_map) = saliency.computeSaliency(image)\n",
    "saliency_map = (saliency_map * 255).astype(\"uint8\")\n",
    "\n",
    "cv2.imshow(\"Saliency Map\", saliency_map)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Step 4: Segmentation by GrabCut\n",
    "mask = np.zeros(image.shape[:2], np.uint8)\n",
    "rect = (50, 50, image.shape[1]-100, image.shape[0]-100)\n",
    "bgdModel = np.zeros((1, 65), np.float64)\n",
    "fgdModel = np.zeros((1, 65), np.float64)\n",
    "cv2.grabCut(image, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "output_image = image * mask2[:, :, np.newaxis]\n",
    "\n",
    "cv2.imshow(\"Segmented Image\", output_image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Step 5: Non-Maximum Suppression (NMS)\n",
    "def nms(boxes, overlapThresh):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    pick = []\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2] + boxes[:,0]\n",
    "    y2 = boxes[:,3] + boxes[:,1]\n",
    "\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "\n",
    "    while len(idxs) > 0:\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "\n",
    "    return boxes[pick].astype(\"int\")\n",
    "\n",
    "# Apply NMS on the initial rects\n",
    "picked_boxes = nms(boxes, 0.3)\n",
    "\n",
    "# Draw final bounding boxes after NMS\n",
    "for (x, y, w, h) in picked_boxes:\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow(\"Final Bounding Boxes after NMS\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8b65fec-c14f-47ff-b35e-ea93452b1725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Anuj/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "Using cache found in C:\\Users\\Anuj/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image shape: (335, 500, 3)\n",
      "Shape after transform and unsqueeze: torch.Size([1, 1, 3, 384, 576])\n",
      "Shape after squeezing: torch.Size([1, 3, 384, 576])\n",
      "Shape after moving to device: torch.Size([1, 3, 384, 576])\n",
      "Shape after interpolation: torch.Size([335, 500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "# Load MiDaS model\n",
    "model_type = \"DPT_Large\"  # Can also be \"DPT_Hybrid\", \"MiDaS_small\", etc.\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "\n",
    "# Load transforms\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "if model_type in [\"DPT_Large\", \"DPT_Hybrid\"]:\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform\n",
    "\n",
    "# Load and prepare the image\n",
    "image_path = r\"F:\\Anuj\\28.jpg\"  # Replace with your image path\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(f\"Loaded image shape: {image.shape}\")\n",
    "\n",
    "# Apply MiDaS transform\n",
    "input_batch = transform(image).unsqueeze(0)\n",
    "print(f\"Shape after transform and unsqueeze: {input_batch.shape}\")\n",
    "\n",
    "# Remove the extra dimension\n",
    "input_batch = input_batch.squeeze(1)\n",
    "print(f\"Shape after squeezing: {input_batch.shape}\")\n",
    "\n",
    "# Move the input and model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device)\n",
    "input_batch = input_batch.to(device)\n",
    "print(f\"Shape after moving to device: {input_batch.shape}\")\n",
    "\n",
    "# Perform depth estimation\n",
    "with torch.no_grad():\n",
    "    prediction = midas(input_batch)\n",
    "\n",
    "# Resize prediction to original image size\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    prediction.unsqueeze(1),\n",
    "    size=image.shape[:2],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ").squeeze()\n",
    "print(f\"Shape after interpolation: {prediction.shape}\")\n",
    "\n",
    "depth_map = prediction.cpu().numpy()\n",
    "\n",
    "# Normalize the depth map for visualization\n",
    "depth_min = depth_map.min()\n",
    "depth_max = depth_map.max()\n",
    "depth_map_normalized = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "\n",
    "# Save and display the depth map\n",
    "depth_map_normalized = (depth_map_normalized * 255).astype(np.uint8)\n",
    "cv2.imwrite(\"depth_map.png\", depth_map_normalized)\n",
    "cv2.imshow(\"Depth Map\", depth_map_normalized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629f209-318b-4436-ae24-41daf1547fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
