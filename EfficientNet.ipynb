{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc64fa-d5bb-4ed4-8cec-ccdc0fa9a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set the correct data directory path\n",
    "data_dir = r\"F:\\Anuj\\food20dataset\"  # Adjust this path as necessary\n",
    "\n",
    "# Verify the dataset path\n",
    "if not os.path.exists(data_dir):\n",
    "    raise FileNotFoundError(f\"Dataset directory '{data_dir}' not found\")\n",
    "\n",
    "# Define transformations for the training set and test set\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "\n",
    "# Define the dataloaders\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'test']}\n",
    "\n",
    "# Define the device (GPU or CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a pre-trained EfficientNet model\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Modify the final layer to match the number of classes in the Indian food dataset\n",
    "num_classes = len(image_datasets['train'].classes)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Define training function\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)  # Get the logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss = running_loss / len(dataloaders['train'].dataset)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
    "        scheduler.step()\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, lr_scheduler, num_epochs=10)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), r\"C:\\Users\\Anuj\\Documents\\efficientnet_food_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e20179e-6983-477e-b5f6-efd94a8f06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"unet_food_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdceca04-5802-497c-a519-145cd354f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the correct data directory path\n",
    "data_dir = r\"F:\\Anuj\\food20dataset\"  # Adjust this path as necessary\n",
    "\n",
    "# Verify the dataset path\n",
    "if not os.path.exists(data_dir):\n",
    "    raise FileNotFoundError(f\"Dataset directory '{data_dir}' not found\")\n",
    "\n",
    "# Define transformations for the test set\n",
    "data_transforms = {\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load the test dataset with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in ['test']}\n",
    "\n",
    "# Define the dataloader\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=False, num_workers=4)\n",
    "               for x in ['test']}\n",
    "\n",
    "# Define the device (GPU or CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model architecture\n",
    "model = models.inception_v3(pretrained=False)\n",
    "num_classes = len(image_datasets['test'].classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(r\"F:\\Anuj\\FoodModelsTested\\unet_food_model.pth\"))\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders['test']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)  # Get the logits only\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]  # Get the logits from the tuple\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    test_loss = running_loss / len(dataloaders['test'].dataset)\n",
    "    test_acc = correct / total\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    \n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}')\n",
    "    print(f'Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}')\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=image_datasets['test'].classes, yticklabels=image_datasets['test'].classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
